{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main2_google-collab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMgoCLcOElePh16rHeJcmvq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hincz-lab/DeepLearning-SCDBiochip/blob/master/Main2_google-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCYix_4fIGgR",
        "outputId": "25496ee4-6922-4aae-d41b-41b1949475a8"
      },
      "source": [
        "!git clone https://github.com/hincz-lab/DeepLearning-SCDBiochip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepLearning-SCDBiochip'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 44 (delta 17), reused 16 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (44/44), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8EtdAzVILHZ",
        "outputId": "e082f0a0-0aeb-4375-a4fa-f7efbb7db087"
      },
      "source": [
        "%cd DeepLearning-SCDBiochip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepLearning-SCDBiochip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u9mlKzxoINXO",
        "outputId": "fd9c5587-98f8-4253-8069-069c7180ab05"
      },
      "source": [
        "!pip install -r requirement.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirement.txt (line 1)) (2.4.3)\n",
            "Collecting tensorflow-gpu==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/99/ac32fd13d56e40d4c3e6150030132519997c0bb1f06f448d970e81b177e5/tensorflow_gpu-2.3.1-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 53kB/s \n",
            "\u001b[?25hCollecting matplotlib==3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/4f/dd381ecf6c6ab9bcdaa8ea912e866dedc6e696756156d8ecc087e20817e2/matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 248kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirement.txt (line 4)) (1.4.1)\n",
            "Collecting numpy==1.18.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.5MB/s \n",
            "\u001b[?25hCollecting numba==0.50.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/2a/c4c9f2ee4edb374af7faa407f66cdc3427489f1c4c4744aeab76230952b7/numba-0.50.1-cp36-cp36m-manylinux2014_x86_64.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 47.9MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.23.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 48.4MB/s \n",
            "\u001b[?25hCollecting scikit-image==0.17.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/ba/53e1bfbdfd0f94514d71502e3acea494a8b4b57c457adbc333ef386485da/scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4MB 190kB/s \n",
            "\u001b[?25hCollecting opencv-python==4.1.0.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/d2/a2dbf83d4553ca6b3701d91d75e42fe50aea97acdc00652dca515749fb5d/opencv_python-4.1.0.25-cp36-cp36m-manylinux1_x86_64.whl (26.6MB)\n",
            "\u001b[K     |████████████████████████████████| 26.6MB 124kB/s \n",
            "\u001b[?25hCollecting gdown==3.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.4.3->-r requirement.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.4.3->-r requirement.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.12.1)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1->-r requirement.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1->-r requirement.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1->-r requirement.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1->-r requirement.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba==0.50.1->-r requirement.txt (line 6)) (50.3.2)\n",
            "Collecting llvmlite<0.34,>=0.33.0.dev0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/37/10d2c4ba0c131cf112d0a8fc97af7a77ca4a769aff626b2bfa2e57e679cf/llvmlite-0.33.0-cp36-cp36m-manylinux1_x86_64.whl (18.3MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3MB 176kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->-r requirement.txt (line 7)) (1.0.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.17.2->-r requirement.txt (line 8)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.17.2->-r requirement.txt (line 8)) (1.1.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.17.2->-r requirement.txt (line 8)) (2020.9.3)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.17.2->-r requirement.txt (line 8)) (2.5)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.17.2->-r requirement.txt (line 8)) (7.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown==3.12.2->-r requirement.txt (line 10)) (3.0.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown==3.12.2->-r requirement.txt (line 10)) (4.41.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.6/dist-packages (from gdown==3.12.2->-r requirement.txt (line 10)) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.17.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image==0.17.2->-r requirement.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.12.2->-r requirement.txt (line 10)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.12.2->-r requirement.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.12.2->-r requirement.txt (line 10)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.12.2->-r requirement.txt (line 10)) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.12.2->-r requirement.txt (line 10)) (1.7.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.4.8)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-cp36-none-any.whl size=9693 sha256=4ef6c404320601d6218686772cf30f064f1e052d17dd4c84ea69b88b90d3ade0\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n",
            "Successfully built gdown\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement tensorflow-estimator<2.5.0,>=2.4.0rc0, but you'll have tensorflow-estimator 2.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, numpy, tensorflow-gpu, matplotlib, llvmlite, numba, threadpoolctl, scikit-learn, scikit-image, opencv-python, gdown\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: numpy 1.19.4\n",
            "    Uninstalling numpy-1.19.4:\n",
            "      Successfully uninstalled numpy-1.19.4\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: llvmlite 0.31.0\n",
            "    Uninstalling llvmlite-0.31.0:\n",
            "      Successfully uninstalled llvmlite-0.31.0\n",
            "  Found existing installation: numba 0.48.0\n",
            "    Uninstalling numba-0.48.0:\n",
            "      Successfully uninstalled numba-0.48.0\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: scikit-image 0.16.2\n",
            "    Uninstalling scikit-image-0.16.2:\n",
            "      Successfully uninstalled scikit-image-0.16.2\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "Successfully installed gdown-3.12.2 llvmlite-0.33.0 matplotlib-3.1.1 numba-0.50.1 numpy-1.18.5 opencv-python-4.1.0.25 scikit-image-0.17.2 scikit-learn-0.23.2 tensorflow-estimator-2.3.0 tensorflow-gpu-2.3.1 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF1QU9rUIPoY"
      },
      "source": [
        "## Import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYQkR1TVIQN8",
        "outputId": "3afed5d9-af17-46a9-fddd-e3e706a5f283"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras as K\r\n",
        "tf.__version__, K.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.3.1', '2.4.3')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI2ptg8nIRTy"
      },
      "source": [
        "\r\n",
        "# load processing func\r\n",
        "import source.toolkit as tk\r\n",
        "import source.dojo_tools as dj\r\n",
        "\r\n",
        "from source.toolkit import list_channels\r\n",
        "from source.toolkit import list_channels_df\r\n",
        "\r\n",
        "# load main class object for monitoring blood cells\r\n",
        "from source.SickleML_Monitor import CountAdheredBloodCells\r\n",
        "\r\n",
        "# loading tools for extracting gdrive data\r\n",
        "import source.load_data_tools as loading_tools\r\n",
        "\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import time\r\n",
        "from tqdm import tqdm\r\n",
        "import sys\r\n",
        "import gdown\r\n",
        "from natsort import natsorted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew6la_eSIUcH"
      },
      "source": [
        "## Download pretrained weights from gdrive:\r\n",
        "Important to note, if you haven't downloaded the pretrained weights (e.g Resnet50 and encoder-decoder model) in your working directory, you must run the below code. Don't worry if it takes several minutes to load because there is several network weights to download. In addition, by default the remaining pretrained models found in the supplementary material is suppressed with quotations. You are also more then welcome the depress the comment quotations and download those weights onto your working directory. Overall, all of the pretrained weights will be saved in automatic folder called \"./Phase2_Pretrained-models/\" or \"./Phase1_Pretrained-models/\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "hj6kaniRISkf",
        "outputId": "4afb1e45-34f7-4557-ca25-c99ee9f939b0"
      },
      "source": [
        "' Resnet50 Xception pretrained weights '\r\n",
        "\r\n",
        "os.makedirs(f'./Phase2_Pretrained-models/Resnet50/', exist_ok = True)\r\n",
        "os.chdir(f'./Phase2_Pretrained-models/Resnet50/') \r\n",
        "\r\n",
        "loading_tools.resnet50_gdrive_()\r\n",
        "    \r\n",
        "os.chdir(f'../')\r\n",
        "os.chdir(f'../')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "' Download encoder-decoder weights '\r\n",
        "\r\n",
        "os.makedirs(f'./Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/', exist_ok = True)\r\n",
        "os.chdir(f'./Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/') \r\n",
        "\r\n",
        "loading_tools.ce_jaccard_gdrive_()\r\n",
        "    \r\n",
        "os.chdir(f'../')\r\n",
        "os.chdir(f'../')\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Below are functions to load the other pretrained models found in the Supplementary materials ....\r\n",
        "(The below models are only optionally)\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "' Vanilla Xception pretrained weights '\r\n",
        "\r\n",
        "os.makedirs(f'./Phase2_Pretrained-models/Vanilla/', exist_ok = True)\r\n",
        "os.chdir(f'./Phase2_Pretrained-models/Vanilla/') \r\n",
        "\r\n",
        "loading_tools.Vanilla_gdrive_()\r\n",
        "\r\n",
        "os.chdir(f'../')\r\n",
        "os.chdir(f'../')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "' Download hr-net weights '\r\n",
        "\r\n",
        "os.makedirs(f'./Phase1_Pretrained-models/hrnet_ce-jaccard_hr-net/', exist_ok = True)\r\n",
        "os.chdir(f'./Phase1_Pretrained-models/hrnet_ce-jaccard_hr-net/') \r\n",
        "\r\n",
        "loading_tools.hrnet_CeJaccard_gdrive_()\r\n",
        "\r\n",
        "os.chdir(f'../')\r\n",
        "os.chdir(f'../')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "' Download Xception pretrained weights '\r\n",
        "\r\n",
        "os.makedirs(f'./Phase2_Pretrained-models/Xception/', exist_ok = True)\r\n",
        "os.chdir(f'./Phase2_Pretrained-models/Xception/') \r\n",
        "\r\n",
        "loading_tools.Xception_gdrive_()\r\n",
        "\r\n",
        "os.chdir(f'../')\r\n",
        "os.chdir(f'../')\r\n",
        "\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start downloading resnet50 net models...\n",
            "Start downloading encoder-decoder Ce-Jaccard net models...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mtewaKMXqdIVmWj1mXKQfYK9Tr5OCD7J\n",
            "To: /content/DeepLearning-SCDBiochip/Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/Phase1_Kfold-model_0.h5\n",
            "63.5MB [00:00, 217MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lQHqgcRCDhmAL5BrxSyfYwtejmpt_rXP\n",
            "To: /content/DeepLearning-SCDBiochip/Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/Phase1_Kfold-model_0.json\n",
            "100%|██████████| 31.0k/31.0k [00:00<00:00, 8.28MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K1cduESQHDozHZYInEE0PmREdvvahiRs\n",
            "To: /content/DeepLearning-SCDBiochip/Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/Phase1_Kfold-model_1.h5\n",
            "63.5MB [00:01, 43.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rwJ9PKUuZAw9ZON9u6P7KC9H05jIT1nZ\n",
            "To: /content/DeepLearning-SCDBiochip/Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/Phase1_Kfold-model_1.json\n",
            "100%|██████████| 31.0k/31.0k [00:00<00:00, 7.25MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Tn0e3OPhmFjZXLksP2reSTls3CFwoXSv\n",
            "To: /content/DeepLearning-SCDBiochip/Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/Phase1_Kfold-model_2.h5\n",
            "63.5MB [00:00, 82.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RrhIiRtWYFGJTpq-IRTXYyldaO0fKXFN\n",
            "To: /content/DeepLearning-SCDBiochip/Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/Phase1_Kfold-model_2.json\n",
            "100%|██████████| 31.0k/31.0k [00:00<00:00, 8.38MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-R1kK7Z89XVSSpraTS2DfT8nFXqek8Ar\n",
            "To: /content/DeepLearning-SCDBiochip/Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/Phase1_Kfold-model_3.h5\n",
            "63.5MB [00:00, 65.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=127f8DRvLuJ33V5_i-TtOZ4-SKrPzsCsT\n",
            "To: /content/DeepLearning-SCDBiochip/Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/Phase1_Kfold-model_3.json\n",
            "100%|██████████| 31.0k/31.0k [00:00<00:00, 6.69MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EWPauaixDCOHcDdN8LyBYj_CrtgCn6MX\n",
            "To: /content/DeepLearning-SCDBiochip/Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/Phase1_Kfold-model_4.h5\n",
            "63.5MB [00:00, 119MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1N2e2sNJ8agWAiEX9Fw4gjEwk5OcFo_RY\n",
            "To: /content/DeepLearning-SCDBiochip/Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/Phase1_Kfold-model_4.json\n",
            "100%|██████████| 31.1k/31.1k [00:00<00:00, 9.52MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\n' Vanilla Xception pretrained weights '\\n\\nos.makedirs(f'./Phase2_Pretrained-models/Vanilla/', exist_ok = True)\\nos.chdir(f'./Phase2_Pretrained-models/Vanilla/') \\n\\nloading_tools.Vanilla_gdrive_()\\n\\nos.chdir(f'../')\\nos.chdir(f'../')\\n\\n\\n\\n' Download hr-net weights '\\n\\nos.makedirs(f'./Phase1_Pretrained-models/hrnet_ce-jaccard_hr-net/', exist_ok = True)\\nos.chdir(f'./Phase1_Pretrained-models/hrnet_ce-jaccard_hr-net/') \\n\\nloading_tools.hrnet_CeJaccard_gdrive_()\\n\\nos.chdir(f'../')\\nos.chdir(f'../')\\n\\n\\n\\n\\n' Download Xception pretrained weights '\\n\\nos.makedirs(f'./Phase2_Pretrained-models/Xception/', exist_ok = True)\\nos.chdir(f'./Phase2_Pretrained-models/Xception/') \\n\\nloading_tools.Xception_gdrive_()\\n\\nos.chdir(f'../')\\nos.chdir(f'../')\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNOLp-jpIZ5c"
      },
      "source": [
        "## Download test whole channel images from gdrive: \r\n",
        "Here you will just download two test channel images into your working directory. Sorry, we cannot share more channels that was used for this study because the remaining channels are patient confidential. Overall, all of the downloaded test channels will be located in the new directory folder called \"./data/Manual-VS-AI/\"\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2iS6cbOIW5x"
      },
      "source": [
        "' Download channel images from gdrive ... '\r\n",
        "\r\n",
        "channel_path= './data/Manual-VS-AI/'\r\n",
        "os.makedirs(channel_path, exist_ok=True)\r\n",
        "os.chdir(channel_path)\r\n",
        "loading_tools.laminin_channel_gdrive_()\r\n",
        "os.chdir('../')\r\n",
        "os.chdir('../')\r\n",
        "channel_filenames  =  os.listdir(channel_path)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D7WLNFVIbqk",
        "outputId": "8363d714-74a4-4714-8cc2-293ac62c9bf6"
      },
      "source": [
        "print(f'Number of channels: {len(channel_filenames)}')\r\n",
        "channel_filenames\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of channels: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['10 Count 384.jpg', '131 count 351.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhZC6z9AIffP"
      },
      "source": [
        "#### Additional Functions ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGPlEGLpIc10"
      },
      "source": [
        "'  Here, we have a function that automatically loads the whole ensemble model (5 neural networks) '\r\n",
        "\r\n",
        "def load_ensembles(Phase1_path_model, Phase2_path_model):\r\n",
        "\r\n",
        "    Phase1_path = './Phase1_Pretrained-models/' + Phase1_path_model + '/'# folder for Phase I \r\n",
        "    Phase1_filenames = natsorted(os.listdir(Phase1_path))\r\n",
        "    Phase1_final_filenames = []\r\n",
        "    for ii in range(len(Phase1_filenames)):\r\n",
        "        if ii % 2 != 0:\r\n",
        "            Phase1_final_filenames.append(Phase1_filenames[ii].replace('.json', ''))\r\n",
        "\r\n",
        "    Phase2_path = './Phase2_Pretrained-models/' + Phase2_path_model + '/'# folder for Phase II\r\n",
        "    Phase2_filenames = natsorted(os.listdir(Phase2_path))\r\n",
        "    Phase2_final_filenames = []\r\n",
        "    for ii in range(len(Phase2_filenames)):\r\n",
        "        if ii % 2 != 0:\r\n",
        "            Phase2_final_filenames.append(Phase2_filenames[ii].replace('.json',''))\r\n",
        "\r\n",
        "\r\n",
        "    Phase1_ensemble = tk.load_zoo(Phase1_path, Phase1_final_filenames) # loading the Phase I ensemble (expect: 7)\r\n",
        "    Phase2_ensemble = tk.load_zoo(Phase2_path, Phase2_final_filenames) # loading the Phase I ensemble (expect: 5)\r\n",
        "    return Phase1_ensemble, Phase2_ensemble\r\n",
        "\r\n",
        "' function for creating dataframes while computing cell counts during inference '\r\n",
        "\r\n",
        "def create_final_df(counts, times):\r\n",
        "    counts_df = pd.DataFrame(counts)\r\n",
        "    counts_df.columns = ['filename', 'def-sRBC', 'nondef-sRBC', 'Other']\r\n",
        "    times_df = pd.DataFrame(times)\r\n",
        "    times_df.columns = ['time_secs']\r\n",
        "    final_df = pd.concat([counts_df, times_df], axis = 1)\r\n",
        "    return final_df\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXMxHpkbIj9c"
      },
      "source": [
        "## Inference: end-to-end predictions\r\n",
        "Here, we will input a whole channel image which will be segmented so that individual adhered cells can be extracted for sRBC morphology classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6-DqtjkIi2O",
        "outputId": "23bdce71-6dd7-4705-f597-74c7e06ce7f4"
      },
      "source": [
        "%%time \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "counts, times = [], [] \r\n",
        "count_container, time_container = [], []\r\n",
        "\r\n",
        "rbc_thres = [0.4]\r\n",
        "wbc_thres = [0.4]\r\n",
        "other_thres = [0.9]\r\n",
        "\r\n",
        "Phase1_names, Phase2_names = 'ce-jaccard_encoder-decoder-net', 'Resnet50'\r\n",
        "\r\n",
        "Phase1_ensemble, Phase2_ensemble = load_ensembles(Phase1_names, Phase2_names)\r\n",
        "counts, times = [], [] \r\n",
        "\r\n",
        "for index, filenames in enumerate(channel_filenames):\r\n",
        "    for rep in ((\".png\", \"\"), (\".jpg\", \"\")):\r\n",
        "        clean_filename = filenames.replace(*rep)\r\n",
        "    print('Analysis:', index, '| Channel:', clean_filename)\r\n",
        "    print('==================================================================')\r\n",
        "    channel = CountAdheredBloodCells(channel_path, filenames) # calling the class object\r\n",
        "    # calling the function to output cell counts\r\n",
        "    start = time.time()\r\n",
        "    sRBC, WBC, Others, img_container, sRBC_container, WBC_container, Other_container = channel.call_pipeline(Phase1_ensemble, Phase2_ensemble, rbc_thres, wbc_thres, other_thres)\r\n",
        "    end = time.time()\r\n",
        "    run_time = end-start\r\n",
        "            \r\n",
        "    times.append([run_time])\r\n",
        "    counts.append([filenames, sRBC, WBC, Others])\r\n",
        "            \r\n",
        "    final_df = create_final_df(counts,times)\r\n",
        "   # final_df.to_csv(f'./AI-vs-Human_counts/{Phase1_name}_{Phase2_name}.csv', index = False)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analysis: 0 | Channel: 10 Count 384\n",
            "==================================================================\n",
            "Prepare the Phase I data ...\n",
            "Total number of extracted tiles:  3500\n",
            "Complete ...\n",
            "Implementing Phase I ...\n",
            "Complete ...\n",
            "Prepare Phase II data ...\n",
            "Complete ...\n",
            "Implementing Phase II ...\n",
            "Complete ...\n",
            "\n",
            "Analysis: 1 | Channel: 131 count 351\n",
            "==================================================================\n",
            "Prepare the Phase I data ...\n",
            "Total number of extracted tiles:  3500\n",
            "Complete ...\n",
            "Implementing Phase I ...\n",
            "Complete ...\n",
            "Prepare Phase II data ...\n",
            "Complete ...\n",
            "Implementing Phase II ...\n",
            "Complete ...\n",
            "\n",
            "CPU times: user 3min 12s, sys: 1min 1s, total: 4min 13s\n",
            "Wall time: 4min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "kHOH00IlInbm",
        "outputId": "ac24cd70-5738-402b-e583-e39a00e42048"
      },
      "source": [
        "\r\n",
        "\r\n",
        "print(\"Here is the predicted counts ...\")\r\n",
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is the predicted counts ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>def-sRBC</th>\n",
              "      <th>nondef-sRBC</th>\n",
              "      <th>Other</th>\n",
              "      <th>time_secs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10 Count 384.jpg</td>\n",
              "      <td>327</td>\n",
              "      <td>33</td>\n",
              "      <td>12</td>\n",
              "      <td>122.586944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>131 count 351.jpg</td>\n",
              "      <td>138</td>\n",
              "      <td>166</td>\n",
              "      <td>16</td>\n",
              "      <td>114.167889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            filename  def-sRBC  nondef-sRBC  Other   time_secs\n",
              "0   10 Count 384.jpg       327           33     12  122.586944\n",
              "1  131 count 351.jpg       138          166     16  114.167889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}