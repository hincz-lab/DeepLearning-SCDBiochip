{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as K \n",
    "tf.__version__, K.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processing func\n",
    "import source.toolkit as tk\n",
    "import source.dojo_tools as dj \n",
    "\n",
    "from source.toolkit import list_channels\n",
    "from source.toolkit import list_channels_df\n",
    "\n",
    "# load main class object for monitoring blood cells\n",
    "from source.SickleML_Monitor import CountAdheredBloodCells \n",
    "\n",
    "# Loading tools for extracting gdrive data \n",
    "import source.load_data_tools as loading_tools\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import gdown "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pretrained weights from gdrive: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to note, if you haven't downloaded the pretrained weights (e.g Resnet50 and encoder-decoder model) in your working directory, you must run the below code. Don't worry if it takes several minutes to load because there is several network weights to download. In addition, by default the remaining pretrained models found in the supplementary material is suppressed with quotations. You are also more then welcome the depress the comment quotations and download those weights onto your working directory. Overall, all of the pretrained weights will be saved in automatic folder called \"./Phase2_Pretrained-models/*\" or \"./Phase1_Pretrained-models/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "' Resnet50 Xception pretrained weights '\n",
    "\n",
    "os.makedirs(f'./Phase2_Pretrained-models/Resnet50/', exist_ok = True)\n",
    "os.chdir(f'./Phase2_Pretrained-models/Resnet50/') \n",
    "\n",
    "loading_tools.resnet50_gdrive_()\n",
    "    \n",
    "os.chdir(f'../')\n",
    "os.chdir(f'../')\n",
    "\n",
    "\n",
    "\n",
    "' Download encoder-decoder weights '\n",
    "\n",
    "os.makedirs(f'./Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/', exist_ok = True)\n",
    "os.chdir(f'./Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/') \n",
    "\n",
    "loading_tools.ce_jaccard_gdrive_()\n",
    "    \n",
    "os.chdir(f'../')\n",
    "os.chdir(f'../')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Below are functions to load the other pretrained models found in the Supplementary materials ....\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "' Vanilla Xception pretrained weights '\n",
    "\n",
    "os.makedirs(f'./Phase2_Pretrained-models/Vanilla/', exist_ok = True)\n",
    "os.chdir(f'./Phase2_Pretrained-models/Vanilla/') \n",
    "\n",
    "loading_tools.Vanilla_gdrive_()\n",
    "\n",
    "os.chdir(f'../')\n",
    "os.chdir(f'../')\n",
    "\n",
    "\n",
    "\n",
    "' Download hr-net weights '\n",
    "\n",
    "os.makedirs(f'./Phase1_Pretrained-models/hrnet_ce-jaccard_hr-net/', exist_ok = True)\n",
    "os.chdir(f'./Phase1_Pretrained-models/hrnet_ce-jaccard_hr-net/') \n",
    "\n",
    "loading_tools.hrnet_CeJaccard_gdrive_()\n",
    "\n",
    "os.chdir(f'../')\n",
    "os.chdir(f'../')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "' Download Xception pretrained weights '\n",
    "\n",
    "os.makedirs(f'./Phase2_Pretrained-models/Xception/', exist_ok = True)\n",
    "os.chdir(f'./Phase2_Pretrained-models/Xception/') \n",
    "\n",
    "loading_tools.Xception_gdrive_()\n",
    "\n",
    "os.chdir(f'../')\n",
    "os.chdir(f'../')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download test whole channel images from gdrive:\n",
    "Here you will just download two test channel images into your working directory. Sorry, we cannot share more channels that was used for this study because the remaining channels are patient confidential. Overall, all of the downloaded test channels will be located in the new directory folder called \"./data/Manual-VS-AI/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' Download channel images from gdrive ... '\n",
    "\n",
    "channel_path= './data/Manual-VS-AI/'\n",
    "os.makedirs(channel_path, exist_ok=True)\n",
    "os.chdir(channel_path)\n",
    "loading_tools.laminin_channel_gdrive_()\n",
    "os.chdir('../')\n",
    "os.chdir('../')\n",
    "channel_filenames  =  os.listdir(channel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of channels: {len(channel_filenames)}')\n",
    "channel_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'  Here, we have a function that automatically loads the whole ensemble model (5 neural networks) '\n",
    "\n",
    "def load_ensembles(Phase1_path_model, Phase2_path_model):\n",
    "\n",
    "    Phase1_path = './Phase1_Pretrained-models/' + Phase1_path_model + '/'# folder for Phase I \n",
    "    Phase1_filenames = os.listdir(Phase1_path)\n",
    "    Phase1_final_filenames = []\n",
    "    for ii in range(len(Phase1_filenames)):\n",
    "        if ii % 2 != 0:\n",
    "            Phase1_final_filenames.append(Phase1_filenames[ii].replace('.json', ''))\n",
    "\n",
    "    Phase2_path = './Phase2_Pretrained-models/' + Phase2_path_model + '/'# folder for Phase II\n",
    "    Phase2_filenames = os.listdir(Phase2_path)\n",
    "    Phase2_final_filenames = []\n",
    "    for ii in range(len(Phase2_filenames)):\n",
    "        if ii % 2 != 0:\n",
    "            Phase2_final_filenames.append(Phase2_filenames[ii].replace('.json',''))\n",
    "\n",
    "\n",
    "    Phase1_ensemble = tk.load_zoo(Phase1_path, Phase1_final_filenames) # loading the Phase I ensemble (expect: 7)\n",
    "    Phase2_ensemble = tk.load_zoo(Phase2_path, Phase2_final_filenames) # loading the Phase I ensemble (expect: 5)\n",
    "    return Phase1_ensemble, Phase2_ensemble\n",
    "\n",
    "' function for creating dataframes while computing cell counts during inference '\n",
    "\n",
    "def create_final_df(counts, times):\n",
    "    counts_df = pd.DataFrame(counts)\n",
    "    counts_df.columns = ['filename', 'def-sRBC', 'nondef-sRBC', 'Other']\n",
    "    times_df = pd.DataFrame(times)\n",
    "    times_df.columns = ['time_secs']\n",
    "    final_df = pd.concat([counts_df, times_df], axis = 1)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: end-to-end predictions \n",
    "\n",
    "Here, we will input a whole channel image which will be segmented so that individual adhered cells can be extracted for sRBC morphology classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "counts, times = [], [] \n",
    "count_container, time_container = [], []\n",
    "\n",
    "rbc_thres = [0.4]\n",
    "wbc_thres = [0.4]\n",
    "other_thres = [0.9]\n",
    "\n",
    "Phase1_names, Phase2_names = 'ce-jaccard_encoder-decoder-net', 'Resnet50'\n",
    "\n",
    "Phase1_ensemble, Phase2_ensemble = load_ensembles(Phase1_names, Phase2_names)\n",
    "counts, times = [], [] \n",
    "\n",
    "for index, filenames in enumerate(channel_filenames):\n",
    "    for rep in ((\".png\", \"\"), (\".jpg\", \"\")):\n",
    "        clean_filename = filenames.replace(*rep)\n",
    "    print('Analysis:', index, '| Channel:', clean_filename)\n",
    "    print('==================================================================')\n",
    "    channel = CountAdheredBloodCells(channel_path, filenames) # calling the class object\n",
    "    # calling the function to output cell counts\n",
    "    start = time.time()\n",
    "    sRBC, WBC, Others, img_container, sRBC_container, WBC_container, Other_container = channel.call_pipeline(Phase1_ensemble, Phase2_ensemble, rbc_thres, wbc_thres, other_thres)\n",
    "    end = time.time()\n",
    "    run_time = end-start\n",
    "            \n",
    "    times.append([run_time])\n",
    "    counts.append([filenames, sRBC, WBC, Others])\n",
    "            \n",
    "    final_df = create_final_df(counts,times)\n",
    "   # final_df.to_csv(f'./AI-vs-Human_counts/{Phase1_name}_{Phase2_name}.csv', index = False)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here is the predicted counts ...\")\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
